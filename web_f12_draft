<div class="tab-pane active" id="learn_the_details-overview">
                    
                        
                        <script>
                            {
                                const content = '\u003Cp\u003E\u003Cstrong\u003E\u003Cspan style\u003D\u0022font\u002Dsize: 1.5em\u003B\u0022\u003EOverview\u003C/span\u003E\u003C/strong\u003E\u003C/p\u003E\u000D\u000A\u003Ccenter\u003E\u000D\u000A\u003Cp\u003E\u003Cimg src\u003D\u0022partial_pcds.gif\u0022 alt\u003D\u0022\u0022 /\u003E\u003C/p\u003E\u000D\u000A\u003C/center\u003E\u000D\u000A\u003Cp\u003ERealistic 3D object modelling, especially from limited observations or random conditions, poses a significant challenge with broad applications in various vision and robotics tasks. To advance research in this field, we present the OmniObject3D dataset, a comprehensive collection of high\u002Dquality, real\u002Dscanned 3D objects with an extensive vocabulary.\u003C/p\u003E\u000D\u000A\u003Cp\u003EIn this challenge, we emphasize two fundamental problems: sparse\u002Dview reconstruction, involving the prediction of novel view images and 3D mesh from a limited set of input images, and 3D object generation, both conditionally and unconditionally.\u003C/p\u003E\u000D\u000A\u003Cp\u003EIn addition to the public training set, we have included a hidden test set \u003Ca href\u003D\u0022https://drive.google.com/file/d/1gbMBT4MBGC5\u002DXMyKHsBq04_c5KKDhLKe/view?usp\u003Ddrive_link\u0022 target\u003D\u0022_blank\u0022\u003Ehere\u003C/a\u003E\u0026nbsp\u003Bspecifically for the sparse\u002Dview reconstruction track. For both tracks, participants must submit their final prediction files based on the provided examples in our \u003Ca href\u003D\u0022https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks\u0022 target\u003D\u0022_blank\u0022\u003Ecodebase\u003C/a\u003E. These submissions will be thoroughly evaluated to determine the winners.\u003C/p\u003E\u000D\u000A\u003Cp\u003E\u0026nbsp\u003B\u003C/p\u003E\u000D\u000A\u003Ch2\u003EDataset\u003C/h2\u003E\u000D\u000A\u003Cp\u003ETo access the OmniObject3D dataset and codebase, please visit our \u003Ca href\u003D\u0022https://omniobject3d.github.io/\u0022 target\u003D\u0022_blank\u0022\u003Ewebsite\u003C/a\u003E, where you can find detailed data descriptions and examples of usage for these two\u0026nbsp\u003Btracks.\u003C/p\u003E\u000D\u000A\u003Cp\u003E\u0026nbsp\u003B\u003C/p\u003E\u000D\u000A\u003Ch2\u003ESubmission\u003C/h2\u003E\u000D\u000A\u003Cp\u003EUsers can participate in\u0026nbsp\u003B\u003Cstrong\u003Eone or both\u003C/strong\u003E\u0026nbsp\u003Bof the following tracks:\u003C/p\u003E\u000D\u000A\u003Ch4\u003ETrack\u002D1 | Sparse\u002Dview Reconstruction\u003C/h4\u003E\u000D\u000A\u003Cp\u003EThis phase evaluates algorithms for novel view synthesis and surface reconstruction given a few posed images of each object.\u0026nbsp\u003BThe number of input images will be 1, 2, and 3, as provided in the test set \u003Ca href\u003D\u0022https://drive.google.com/file/d/1gbMBT4MBGC5\u002DXMyKHsBq04_c5KKDhLKe/view?usp\u003Ddrive_link\u0022 target\u003D\u0022_blank\u0022\u003Ehere\u003C/a\u003E.\u0026nbsp\u003BSubmit the predicted novel view images and extracted point clouds in a .zip file. Please refer to the\u0026nbsp\u003Btools and provided examples \u003Ca href\u003D\u0022https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks/sparse_view_reconstruction\u0022 target\u003D\u0022_blank\u0022\u003Ehere\u003C/a\u003E\u0026nbsp\u003Band carefully check the format to ensure a successful submission.\u003C/p\u003E\u000D\u000A\u003Ch4\u003ETrack\u002D2 | 3D Object Generation\u003C/h4\u003E\u000D\u000A\u003Cp\u003EThis phase evaluates algorithms for realistic 3D object generation on the OmniObject3D dataset. Submit the post\u002Dprocessed results on the generated objects in a .zip files.\u0026nbsp\u003BPlease refer to the\u0026nbsp\u003Btools and provided examples \u003Ca href\u003D\u0022https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks/3d_generation\u0022 target\u003D\u0022_blank\u0022\u003Ehere\u003C/a\u003E\u0026nbsp\u003Band carefully check the format to ensure a successful submission.\u003C/p\u003E\u000D\u000A\u003Cp\u003E\u0026nbsp\u003B\u003C/p\u003E\u000D\u000A\u003Ch2\u003ETimeline\u003C/h2\u003E\u000D\u000A\u003Cul\u003E\u000D\u000A\u003Cli\u003EAug. 01, 2023 \u002D Submission start date\u003C/li\u003E\u000D\u000A\u003Cli\u003ESep. 08, 2023 \u002D Track\u002D2 public submission deadline\u003C/li\u003E\u000D\u000A\u003Cli\u003ESep. 12, 2023 \u002D Track\u002D2 private submission deadline\u003C/li\u003E\u000D\u000A\u003Cli\u003ESep. 15, 2023 \u002D Track\u002D1 public submission deadline\u003C/li\u003E\u000D\u000A\u003Cli\u003ESep. 22, 2021 \u002D Technical report, source code, and pre\u002Dtrained model deadline\u003C/li\u003E\u000D\u000A\u003Cli\u003EOct. 02, 2021 \u002D Awards at ICCV Workshop\u003C/li\u003E\u000D\u000A\u003C/ul\u003E\u000D\u000A\u003Ch3\u003EReminders\u003C/h3\u003E\u000D\u000A\u003Cp\u003E1. Public submission for Track\u002D2 (3D object generation) is closed by Sep. 08\u003B during the period of private submission (Sep. 08 \u002D 12),\u0026nbsp\u003B the \u003Cstrong\u003ETOP\u002D10\u003C/strong\u003E of participants in this track will be notified and are required to\u0026nbsp\u003B\u003Cstrong\u003Esubmit the raw data before post\u002Dprocessing during the public submission period\u0026nbsp\u003B(rendered images\u003C/strong\u003E\u0026nbsp\u003B\u003Cstrong\u003E\u0026amp\u003B\u0026nbsp\u003B3D objects)\u0026nbsp\u003B\u003C/strong\u003Eto us for a comprehensive\u0026nbsp\u003B\u003Cstrong\u003Euser study\u003C/strong\u003E. The results of user study will serve as the most critical metric for the determination of winners.\u003C/p\u003E\u000D\u000A\u003Cp\u003E2. By Sep. 22, the \u003Cstrong\u003ETOP\u002D5 participants in Track\u002D1\u003C/strong\u003E and \u003Cstrong\u003ETOP\u002D10 participants in Track\u002D2\u003C/strong\u003E\u0026nbsp\u003Bare required to submit a short technical report (2\u002D4\u0026nbsp\u003Bpages of ICCV two\u002Dcolumn template that briefly introduces their technical contributions and analysis of the results), source code, pre\u002Dtrained model of their methods. The code would\u0026nbsp\u003B\u003Cstrong\u003EONLY\u003C/strong\u003E\u0026nbsp\u003Bbe used for verifying the legality of the algorithm and would\u0026nbsp\u003B\u003Cstrong\u003ENOT\u003C/strong\u003E\u0026nbsp\u003Bbe further distributed. The legacy of their method and its reproducibility will also be crucial factors in determining the winners.\u003C/p\u003E\u000D\u000A\u003Cp\u003E\u0026nbsp\u003B\u003C/p\u003E\u000D\u000A\u003Ch2\u003EGeneral Rules\u003C/h2\u003E\u000D\u000A\u003Cp\u003EPlease check the\u0026nbsp\u003B\u003Cstrong\u003Eterms and conditions\u003C/strong\u003E\u0026nbsp\u003Bfor further rules and details.\u003C/p\u003E\u000D\u000A\u003Cp\u003E\u0026nbsp\u003B\u003C/p\u003E\u000D\u000A\u003Ch2\u003EContact Us\u003C/h2\u003E\u000D\u000A\u003Cp\u003EIf you have any questions, please contact us by raising an issue on our\u0026nbsp\u003B\u003Ca href\u003D\u0022https://github.com/omniobject3d/OmniObject3D\u0022 target\u003D\u0022_blank\u0022\u003Egithub project\u003C/a\u003E.\u003C/p\u003E';
                                document.write(DOMPurify.sanitize(
                                    content,
                                    { FORBID_TAGS: ['style'] }
                                ));
                            }
                        </script><p><strong><span style="font-size: 1.5em;">Overview</span></strong></p>
<center>
<p><img alt="" src="partial_pcds.gif"></p>
</center>
<p>Realistic 3D object modelling, especially from limited observations or random conditions, poses a significant challenge with broad applications in various vision and robotics tasks. To advance research in this field, we present the OmniObject3D dataset, a comprehensive collection of high-quality, real-scanned 3D objects with an extensive vocabulary.</p>
<p>In this challenge, we emphasize two fundamental problems: sparse-view reconstruction, involving the prediction of novel view images and 3D mesh from a limited set of input images, and 3D object generation, both conditionally and unconditionally.</p>
<p>In addition to the public training set, we have included a hidden test set <a href="https://drive.google.com/file/d/1gbMBT4MBGC5-XMyKHsBq04_c5KKDhLKe/view?usp=drive_link">here</a>&nbsp;specifically for the sparse-view reconstruction track. For both tracks, participants must submit their final prediction files based on the provided examples in our <a href="https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks">codebase</a>. These submissions will be thoroughly evaluated to determine the winners.</p>
<p>&nbsp;</p>
<h2>Dataset</h2>
<p>To access the OmniObject3D dataset and codebase, please visit our <a href="https://omniobject3d.github.io/">website</a>, where you can find detailed data descriptions and examples of usage for these two&nbsp;tracks.</p>
<p>&nbsp;</p>
<h2>Submission</h2>
<p>Users can participate in&nbsp;<strong>one or both</strong>&nbsp;of the following tracks:</p>
<h4>Track-1 | Sparse-view Reconstruction</h4>
<p>This phase evaluates algorithms for novel view synthesis and surface reconstruction given a few posed images of each object.&nbsp;The number of input images will be 1, 2, and 3, as provided in the test set <a href="https://drive.google.com/file/d/1gbMBT4MBGC5-XMyKHsBq04_c5KKDhLKe/view?usp=drive_link">here</a>.&nbsp;Submit the predicted novel view images and extracted point clouds in a .zip file. Please refer to the&nbsp;tools and provided examples <a href="https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks/sparse_view_reconstruction">here</a>&nbsp;and carefully check the format to ensure a successful submission.</p>
<h4>Track-2 | 3D Object Generation</h4>
<p>This phase evaluates algorithms for realistic 3D object generation on the OmniObject3D dataset. Submit the post-processed results on the generated objects in a .zip files.&nbsp;Please refer to the&nbsp;tools and provided examples <a href="https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks/3d_generation">here</a>&nbsp;and carefully check the format to ensure a successful submission.</p>
<p>&nbsp;</p>
<h2>Timeline</h2>
<ul>
<li>Aug. 01, 2023 - Submission start date</li>
<li>Sep. 08, 2023 - Track-2 public submission deadline</li>
<li>Sep. 12, 2023 - Track-2 private submission deadline</li>
<li>Sep. 15, 2023 - Track-1 public submission deadline</li>
<li>Sep. 22, 2021 - Technical report, source code, and pre-trained model deadline</li>
<li>Oct. 02, 2021 - Awards at ICCV Workshop</li>
</ul>
<h3>Reminders</h3>
<p>1. Public submission for Track-2 (3D object generation) is closed by Sep. 08; during the period of private submission (Sep. 08 - 12),&nbsp; the <strong>TOP-10</strong> of participants in this track will be notified and are required to&nbsp;<strong>submit the raw data before post-processing during the public submission period&nbsp;(rendered images</strong>&nbsp;<strong>&amp;&nbsp;3D objects)&nbsp;</strong>to us for a comprehensive&nbsp;<strong>user study</strong>. The results of user study will serve as the most critical metric for the determination of winners.</p>
<p>2. By Sep. 22, the <strong>TOP-5 participants in Track-1</strong> and <strong>TOP-10 participants in Track-2</strong>&nbsp;are required to submit a short technical report (2-4&nbsp;pages of ICCV two-column template that briefly introduces their technical contributions and analysis of the results), source code, pre-trained model of their methods. The code would&nbsp;<strong>ONLY</strong>&nbsp;be used for verifying the legality of the algorithm and would&nbsp;<strong>NOT</strong>&nbsp;be further distributed. The legacy of their method and its reproducibility will also be crucial factors in determining the winners.</p>
<p>&nbsp;</p>
<h2>General Rules</h2>
<p>Please check the&nbsp;<strong>terms and conditions</strong>&nbsp;for further rules and details.</p>
<p>&nbsp;</p>
<h2>Contact Us</h2>
<p>If you have any questions, please contact us by raising an issue on our&nbsp;<a href="https://github.com/omniobject3d/OmniObject3D">github project</a>.</p>
                        
                    
                </div>



                <div class="tab-pane active" id="learn_the_details-evaluation">
                    
                        
                        <script>
                            {
                                const content = '\u003Ch3\u003EEvaluation Criteria\u003C/h3\u003E\u000D\u000A\u003Ch3\u003ETrack\u002D1 | Sparse\u002DView Reconstruction\u003C/h3\u003E\u000D\u000A\u003Cp\u003EWe evaluate the novel view synthesis and 3D reconstruction accuracy by computing the PSNR on 10 test views and Chamfer Distance (CD) between the reconstruction and the ground truth, respectively. PSNR would be the main metric for the TOP\u002D5 selection, while CD (under the standard point cloud sampling resolution provided in our codebase) would also be an important factor for the determination of the final award winners. The results are averaged across the whole test set for overall evaluation criteria.\u003C/p\u003E\u000D\u000A\u003Ch3\u003ETrack\u002D2 | 3D Object Generation\u003C/h3\u003E\u000D\u000A\u003Cp\u003EDuring the public submission stage, we evaluate the generation performance by computing the FID score, which would be the main metric for the TOP\u002D10 selection. For the determination of the final award winners, user study after the private submission would be the most important factor, and we would also take quantitative evaluation, including\u0026nbsp\u003BFID, Cov, and MMD, into consideration.\u0026nbsp\u003B\u003C/p\u003E';
                                document.write(DOMPurify.sanitize(
                                    content,
                                    { FORBID_TAGS: ['style'] }
                                ));
                            }
                        </script><h3>Evaluation Criteria</h3>
<h3>Track-1 | Sparse-View Reconstruction</h3>
<p>We evaluate the novel view synthesis and 3D reconstruction accuracy by computing the PSNR on 10 test views and Chamfer Distance (CD) between the reconstruction and the ground truth, respectively. PSNR would be the main metric for the TOP-5 selection, while CD (under the standard point cloud sampling resolution provided in our codebase) would also be an important factor for the determination of the final award winners. The results are averaged across the whole test set for overall evaluation criteria.</p>
<h3>Track-2 | 3D Object Generation</h3>
<p>During the public submission stage, we evaluate the generation performance by computing the FID score, which would be the main metric for the TOP-10 selection. For the determination of the final award winners, user study after the private submission would be the most important factor, and we would also take quantitative evaluation, including&nbsp;FID, Cov, and MMD, into consideration.&nbsp;</p>
                        
                    
                </div>